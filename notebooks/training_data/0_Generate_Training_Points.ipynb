{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bb016a6-d175-4017-84f4-92290942069c",
   "metadata": {},
   "source": [
    "# QA Training Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332b748",
   "metadata": {},
   "source": [
    "# Create LULC training data from existing LULC products.\n",
    "\n",
    "Key considerations:\n",
    "* Spatial distribution, especially in SIDS\n",
    "* Class distribution\n",
    "* Temporal coverage\n",
    "* Quality and confidence\n",
    "* Trends in agreement with LULC datasets\n",
    "\n",
    "References\n",
    "- https://github.com/frontiersi/FAO_LC_workshop_Rwanda/blob/main/0_Generate_Training_Points.ipynb\n",
    "- https://github.com/frontiersi/land_cover_mapping_DEAfrica/blob/main/notebooks/Lesotho/Filter_training_data_GEE_replicate.ipynb\n",
    "- https://github.com/auspatious/ldn-lulc/blob/main/notebooks/Compare_LULC.ipynb\n",
    "- https://github.com/auspatious/ldn-lulc/blob/main/notebooks/Compare_LULC_per_class.ipynb\n",
    "\n",
    "Steps\n",
    "1. Extract points with reliable LU/LC classes from existing products. Use these 3 https://github.com/auspatious/ldn-lulc/blob/main/notebooks/Compare_LULC_per_class.ipynb. ESA Worldcover is the best one. Do this for Singaport or Suva, Fiji bounding box (due to presence of all classes hopefully). Classes should have roughly equal proportions in the sample points. Random sample all classes, use cluster method to reject outliers. This tells you which training points are more reliable.\n",
    "2. Align typologies/classes from those products to our classes defined here (level 1 only) https://github.com/auspatious/ldn-lulc/blob/8dd22d4190207c977c55e4cde9f236df6daf8722/typology/typology.md\n",
    "3. Extract the observational data (RGB and all other bands incl. geomad attributes), indices (NDVI, etc. TBC), DEM, etc.) This partially relies on geomad.\n",
    "4. Then iterate and tune the method to extract reliable training data.\n",
    "\n",
    "Output schema of training data: coordinates, crs, year, class, source of class e.g. \"ESA Worldcover\", RGB, all other bands, geomad properties, indices (e.g. NDVI), elevation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9bfe1-7aec-4eb2-b687-6b2a3e3e22a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Forked from https://github.com/frontiersi/FAO_LC_workshop_Rwanda/blob/main/0_Generate_Training_Points.ipynb and https://github.com/auspatious/ldn-lulc/blob/main/notebooks/Compare_LULC_per_class.ipynb\n",
    "\n",
    "## Background\n",
    "\n",
    "**Training data** is the most important part of any supervised machine learning workflow. The quality of the training data has a greater impact on the classification than the algorithm used. Large and accurate training data sets are preferable: increasing the training sample size results in increased classification accuracy ([Maxell et al 2018](https://www.tandfonline.com/doi/full/10.1080/01431161.2018.1433343)).  A review of training data methods in the context of Earth Observation is available [here](https://www.mdpi.com/2072-4292/12/6/1034).\n",
    "\n",
    "There are many platforms to use for gathering land cover training labels, the best one to use depends on your application. GIS platforms are great for collecting training data as they are highly flexible and mature platforms; [Geo-Wiki](https://www.geo-wiki.org/) and [Collect Earth Online](https://collect.earth/home) are two open-source websites that may also be useful depending on the reference data strategy employed. Alternatively, there are many pre-existing training datasets on the web that may be useful, e.g. [Radiant Earth](https://www.radiant.earth/) manages a growing number of reference datasets for use by anyone. With locations of land cover labels available, we can extract features at these locations from satellite imagery as input for machine learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7deb13-06df-4f10-b313-9812e2c765ea",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "As timely training data is not always available, in this notebook we demonstrate how to generate a set of randomly distributed training points for a selected AOI from an existing classification raster.\n",
    "\n",
    "The workflow includes the following steps:\n",
    "\n",
    "1. Preview the AOI (Singapore) on a basemap. Define datetime/year of interest.\n",
    "2. Use 3 existing LU/LC products. Only use data where they agree. 30m resolution.\n",
    "4. Remap/merge classes on the classification raster to our typologies.\n",
    "5. Generate randomly distributed training points and export for future use.\n",
    "6. Search and load GeoMAD, elevation.\n",
    "7. Scale GeoMedian values and calculate indices.\n",
    "8. Add GeoMAD, elevation and indices data to training data points. Export.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a84f8-7d62-4dba-af3a-619f3fba0069",
   "metadata": {},
   "source": [
    "### Load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2df8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload functions during development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428a717-3694-4d1e-b615-c3f5b0ba7cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import rioxarray  # noqa: F401\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "from odc.geo.geom import BoundingBox, Geometry\n",
    "from odc.stac import load\n",
    "from planetary_computer import sign_url\n",
    "from pystac import Item\n",
    "from pystac.client import Client\n",
    "from rustac import search_sync\n",
    "from scipy.ndimage import uniform_filter\n",
    "\n",
    "from ldn.random_sampling import random_sampling  # adapted from function by Chad Burton: https://gist.github.com/cbur24/04760d645aa123a3b1817b07786e7d9f\n",
    "from ldn.typology import classes, colors, world_cover_map, cci_lc_map, io_map\n",
    "from notebooks.src.Compare_LULC_func import standardise_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c7a9a5-ce0f-4eac-bad6-0896c1aecb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime = '2020-06'\n",
    "datetime_year = datetime.split(\"-\")[0]\n",
    "lulc_resolution = 30 # meters. CCI: 300m, WC: 10m, IO: 10m. Meet in the middle with 30m.\n",
    "lulc_class_raster = f'singapore_agreeing_lulc_{datetime}_{lulc_resolution}m.tif'\n",
    "analysis_crs = 'EPSG:6933' # Equal Earth for global analysis\n",
    "output_crs = 'EPSG:4326' # WGS84\n",
    "# output_crs = 'EPSG:32648' # UTM Zone 48N for Singapore\n",
    "class_attr='lulc'\n",
    "\n",
    "bbox = BoundingBox(\n",
    "    # TODO: Viti Levu, Fiji\n",
    "    \n",
    "    # Singapore\n",
    "    left=103.6,\n",
    "    bottom=1.2,\n",
    "    right=104.1,\n",
    "    top=1.48,\n",
    "    crs=output_crs,\n",
    ")\n",
    "bbox.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58bae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clip AOI bounding box to land. Too many ocean water points in the training data that are not relevant for LU/LC (and confuse the terrestrial water).\n",
    "# # TODO: Find a more general way to get land for clipping. This only works for Singapore.\n",
    "# import requests\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import box\n",
    "# from io import BytesIO\n",
    "\n",
    "# # 1. Download GeoJSON\n",
    "# url = \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_SGP_0.json\"\n",
    "# response = requests.get(url)\n",
    "# response.raise_for_status()\n",
    "\n",
    "# # 2. Read into GeoDataFrame\n",
    "# gdf = gpd.read_file(BytesIO(response.content))\n",
    "\n",
    "# # 3. Buffer 200m — project to a metric CRS first, then back\n",
    "# gdf_projected = gdf.to_crs(analysis_crs)\n",
    "# gdf_buffered = gdf_projected.copy()\n",
    "# gdf_buffered[\"geometry\"] = gdf_projected.buffer(200)\n",
    "# gdf_buffered = gdf_buffered.to_crs(output_crs)\n",
    "\n",
    "# # 4. Create bbox as MultiPolygon and clip to buffered land\n",
    "# bbox_shape = box(bbox.left, bbox.bottom, bbox.right, bbox.top)\n",
    "# bbox_gdf = gpd.GeoDataFrame(geometry=[bbox_shape], crs=output_crs)\n",
    "\n",
    "# aoi = gpd.clip(bbox_gdf, gdf_buffered)\n",
    "\n",
    "# aoi.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method of clipping the bbox to land works globally.\n",
    "\n",
    "import geopandas as gpd\n",
    "import duckdb\n",
    "from shapely import wkt\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"INSTALL spatial; LOAD spatial;\")\n",
    "\n",
    "land_sgp = con.execute(f\"\"\"\n",
    "    SELECT\n",
    "        id,\n",
    "        names['primary'] AS primary_name,\n",
    "        ST_AsText(geometry) AS geometry\n",
    "    FROM\n",
    "        read_parquet('s3://overturemaps-us-west-2/release/2026-02-18.0/theme=base/type=land/*', filename=true, hive_partitioning=1)\n",
    "    WHERE\n",
    "        bbox.xmin >= {bbox.left}\n",
    "        AND bbox.xmax <= {bbox.right}\n",
    "        AND bbox.ymin >= {bbox.bottom}\n",
    "        AND bbox.ymax <= {bbox.top}\n",
    "\"\"\").df()\n",
    "\n",
    "land_gdf = (\n",
    "    gpd.GeoDataFrame(\n",
    "        land_sgp,\n",
    "        geometry=land_sgp['geometry'].apply(wkt.loads),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    .to_crs(analysis_crs)\n",
    ")\n",
    "\n",
    "# Buffer 200m and dissolve to clean up messy/overlapping polygons\n",
    "land_buffered = (\n",
    "    land_gdf\n",
    "    .buffer(200)\n",
    "    .union_all()\n",
    ")\n",
    "\n",
    "aoi = gpd.GeoDataFrame(geometry=[land_buffered], crs=analysis_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832fecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi.explore()\n",
    "# print(aoi.to_json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a017603",
   "metadata": {},
   "source": [
    "## Use 3 LU/LC products. Find agreement between them.\n",
    "\n",
    "TODO: Experiment with data quality bands - some LULC datasets have quality information which could be used as additional filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb782b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Planetary Computer STAC API\n",
    "catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1/\")\n",
    "aoi_geojson = json.loads(aoi.to_json(to_wgs84=True))[\"features\"][0][\"geometry\"]\n",
    "\n",
    "# A generic function to load a product given the boundary\n",
    "def load_lulc_data(product: str, resolution = 100):\n",
    "    items = catalog.search(\n",
    "        collections=[product],\n",
    "        intersects=aoi_geojson,\n",
    "        datetime=datetime,\n",
    "    ).item_collection()\n",
    "\n",
    "    print(f\"Found {len(items)} items for product {product}\")\n",
    "\n",
    "    ds = load(\n",
    "        items,\n",
    "        intersects=aoi_geojson,\n",
    "        crs=analysis_crs,\n",
    "        groupby=\"solar_day\",\n",
    "        resolution=resolution,\n",
    "        chunks={\"x\": 2048, \"y\": 2048},\n",
    "        patch_url=sign_url,\n",
    "        resample_alg=\"nearest\",\n",
    "    )\n",
    "\n",
    "    print(f\"Product {product} has variables: {ds.data_vars}\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cci_30m = load_lulc_data('esa-cci-lc', resolution=lulc_resolution) # 300m is native.\n",
    "vars = list(cci_30m.data_vars)\n",
    "cci_30m[\"esa_cci_lc\"] = cci_30m[\"lccs_class\"]\n",
    "ds_tmp = cci_30m.drop_vars(vars)\n",
    "cci_30m = ds_tmp.squeeze().load() # removes any singleton dimension and then load the full data\n",
    "\n",
    "# Clip to AOI. ODC STAC Load intersects param doesn't clip.\n",
    "cci_30m = cci_30m.rio.clip(aoi.to_crs(analysis_crs).geometry, crs=analysis_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ef5e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cci_30m[\"esa_cci_lc\"].odc.explore()\n",
    "# cci_30m[\"esa_cci_lc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d2a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_30m = load_lulc_data('esa-worldcover', resolution=lulc_resolution) # 10m is native.\n",
    "vars = list(wc_30m.data_vars)\n",
    "wc_30m[\"esa_worldcover\"] = wc_30m[\"map\"]\n",
    "ds_tmp = wc_30m.drop_vars(vars)\n",
    "wc_30m = ds_tmp.squeeze().load() #removes any singleton dimension and then load the full data\n",
    "\n",
    "# Clip to AOI. ODC STAC Load intersects param doesn't clip.\n",
    "wc_30m = wc_30m.rio.clip(aoi.to_crs(analysis_crs).geometry, crs=analysis_crs)\n",
    "\n",
    "# Ensure alignment.\n",
    "# TODO: Experiment with nearest resampling instead of majority vote\n",
    "wc_30m[\"esa_worldcover\"] = wc_30m[\"esa_worldcover\"].rio.reproject_match(\n",
    "    cci_30m[\"esa_cci_lc\"], resampling=rasterio.enums.Resampling.mode  # majority vote (the value that appears most often)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3389e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wc_30m[\"esa_worldcover\"].odc.explore()\n",
    "wc_30m[\"esa_worldcover\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8624fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "io_30m = load_lulc_data('io-lulc-annual-v02', resolution=lulc_resolution) # 10m is native.\n",
    "vars = list(io_30m.data_vars)\n",
    "io_30m[\"io_lulc\"] = io_30m[\"data\"]\n",
    "ds_tmp = io_30m.drop_vars(vars)\n",
    "io_30m = ds_tmp.squeeze().load() # removes any singleton dimension and then load the full data\n",
    "\n",
    "# Clip to AOI. ODC STAC Load intersects param doesn't clip.\n",
    "io_30m = io_30m.rio.clip(aoi.to_crs(analysis_crs).geometry, crs=analysis_crs)\n",
    "\n",
    "# Ensure alignment.\n",
    "# TODO: Experiment with nearest resampling instead of majority vote\n",
    "io_30m[\"io_lulc\"] = io_30m[\"io_lulc\"].rio.reproject_match(\n",
    "    cci_30m[\"esa_cci_lc\"], resampling=rasterio.enums.Resampling.mode  # majority vote (the value that appears most often)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9152fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "io_30m[\"io_lulc\"].odc.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the classes to match the typology\n",
    "cci_30m['esa_cci_lc'] = standardise_class(cci_30m['esa_cci_lc'], cci_lc_map)\n",
    "wc_30m['esa_worldcover'] = standardise_class(wc_30m['esa_worldcover'], world_cover_map)\n",
    "io_30m['io_lulc'] = standardise_class(io_30m['io_lulc'], io_map)\n",
    "wc_30m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d219f8b",
   "metadata": {},
   "source": [
    "# Create layer where all 3 land cover products agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_threshold = 40\n",
    "\n",
    "# TODO: Sample pixels with full agreement (and also their neighbors fully agree).\n",
    "# Make a mask of pixels where all three products agree, then erode it by one pixel to ensure neighbors also agree, then sample from that.\n",
    "# Current agreement selection is done on 5x5 grid.\n",
    "\n",
    "# Valid pixels — all three products have data\n",
    "valid = (wc_30m['esa_worldcover'] > 0) & (cci_30m['esa_cci_lc'] > 0) & (io_30m['io_lulc'] > 0)\n",
    "\n",
    "# All-three agreement at each pixel (single boolean, no pairwise steps)\n",
    "all_agree = (\n",
    "    (wc_30m['esa_worldcover'] == cci_30m['esa_cci_lc']) &\n",
    "    (cci_30m['esa_cci_lc'] == io_30m['io_lulc'])\n",
    ").astype(\"float32\").where(valid)\n",
    "\n",
    "# 5x5 local agreement score\n",
    "valid_f = valid.astype(\"float32\").values\n",
    "local_agree = xr.DataArray(\n",
    "    np.where(\n",
    "        (w := uniform_filter(valid_f, size=5, mode=\"nearest\")) > 0,\n",
    "        (uniform_filter(all_agree.fillna(0).values, size=5, mode=\"nearest\") / w) * 100,\n",
    "        np.nan,\n",
    "    ),\n",
    "    coords=all_agree.coords,\n",
    "    dims=all_agree.dims,\n",
    "    name=\"local_agreement_all_5x5\",\n",
    ")\n",
    "\n",
    "# Output: agreed class value where threshold is met, else 0\n",
    "agreed_class = wc_30m['esa_worldcover'].where(\n",
    "    (local_agree >= agreement_threshold) & valid, other=0\n",
    ").rename(class_attr)\n",
    "\n",
    "_classes, counts = np.unique(agreed_class.values, return_counts=True)\n",
    "counts_df = pd.DataFrame({\"class\": _classes, \"pixel_count\": counts})\n",
    "print(counts_df)\n",
    "print(agreed_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c58789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write agreeing pixels as tiff.\n",
    "agreed_class.rio.to_raster(lulc_class_raster, dtype='uint8', compress='deflate')\n",
    "\n",
    "# # Read the tiff\n",
    "# classification_map=xr.open_dataset(lulc_class_raster,engine=\"rasterio\").astype(np.uint8)\n",
    "# classification_map=classification_map.to_array().squeeze()\n",
    "classification_map = agreed_class\n",
    "classification_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc45c7",
   "metadata": {},
   "source": [
    "# Visualise the agreeing classes from 3 products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b82305-cb3b-4f60-8b00-232be9f2bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,1)\n",
    "\n",
    "# Plot classification map\n",
    "unique_values=np.unique(classification_map)\n",
    "cmap=ListedColormap([colors[k] for k in unique_values])\n",
    "norm = BoundaryNorm(list(unique_values)+[np.max(unique_values)+1], cmap.N)\n",
    "classification_map.plot.imshow(ax=axes, \n",
    "                   cmap=cmap,\n",
    "                   norm=norm,\n",
    "                   add_labels=True, \n",
    "                   add_colorbar=False,\n",
    "                   interpolation='none')\n",
    "# add colour legend\n",
    "patches_list=[Patch(facecolor=colour) for colour in colors.values()]\n",
    "axes.legend(patches_list, list(classes.keys()),loc='upper center', ncol =4, bbox_to_anchor=(0.5, -0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49952952-0c9c-4108-b73e-c58e29918267",
   "metadata": {},
   "source": [
    "## Generate random training samples\n",
    "We generate some randomly distributed samples for each class from the clipped classification map using the `random_sampling` function. This function takes in a few parameters:  \n",
    "* `da`: a classified map in the format of 2-dimensional xarray.DataArray\n",
    "* `n`: total number of points to sample.\n",
    "* `min_sample_n`: Minimum number of samples to generate per class if proportional number is smaller than this. **Note that the resultant number of samples may be higher than the set `n` due to setting of this minimum number of samples.** \n",
    "* `sampling`: the sampling strategy, e.g. 'stratified_random' where each class has a number of points proportional to its relative area, 'equal_stratified_random' where each class has the same number of points, or 'manual' which allows you to define number of samples for each class.\n",
    "* `out_fname`: a filepath name for the function to export a shapefile/geojson of the sampling points into a file. You can set this to `None` if you don't need to output the file.\n",
    "* `class_attr`: This is the column name of output dataframe that contains the integer class values on the classified map. \n",
    "* `drop_value`: pixel value on the classification map to be excluded from sampling.\n",
    "\n",
    "The output of the function is a geopandas dataframe of randomly distributed points containing a column `class_attr` identifying class values. \n",
    "\n",
    "Here we extract around 1000 training points in total and export the points in a geojson file for use in the rest of workflow. Here we use the stratified sampling method by setting 'equal_stratified_random', but also set the minimum number of samples as 75 to avoid missing samples for some minor classes. \n",
    "\n",
    "As mentioned earlier we don't want the abandoned classes to be included in the samples we set drop_value as 0 before implementing the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfce700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject to WGS84 so sample points are in lat/lon\n",
    "classification_map_4326 = classification_map.rio.reproject(output_crs)\n",
    "classification_map_4326.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5992659-25c7-484d-9861-432f433744e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fname='training_data.geojson'\n",
    "n=2100\n",
    "min_sample_n=300\n",
    "drop_value=0\n",
    "gpd_random_samples=random_sampling(da=classification_map_4326,n=n,sampling='stratified_random',\n",
    "                                   min_sample_n=min_sample_n,out_fname=None,class_attr=class_attr,drop_value=drop_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a16efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CRS: {gpd_random_samples.crs}\")\n",
    "gpd_random_samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb812c81",
   "metadata": {},
   "source": [
    "## Visualise the training data by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b326905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd_random_samples.explore(\n",
    "    column=class_attr,\n",
    "    categorical=True,\n",
    "    categories=(present_classes := sorted(gpd_random_samples[class_attr].unique())),\n",
    "    cmap=[colors[c] for c in present_classes],\n",
    "    legend=True,\n",
    "    style_kwds={\"radius\": 6, \"fillOpacity\": 0.8, \"weight\": 0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a93afb",
   "metadata": {},
   "source": [
    "## Extract Geomedian/GeoMAD values at training data point locations.\n",
    "\n",
    "For now we run and load individual tiles and mosaic them because we have not run all tiles yet.\n",
    "\n",
    "Once we have finalised the GeoMAD data, we will set up a GeoParquet STAC index for it. Then querying it by bbox will be simple and replace the more complicated process implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142786f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query our GeoMAD STAC-Geoparquet by AOI bbox and datetime.\n",
    "# parquet = \"s3://data.ldn.auspatious.com/ausp_ls_geomad/0-0-2/ausp_ls_geomad.parquet\"\n",
    "stac_geoparquet = \"https://s3.us-west-2.amazonaws.com/data.ldn.auspatious.com/ci_ls_geomad/0-0-2/ci_ls_geomad.parquet\" # Non-Pacific\n",
    "# stac_geoparquet = \"https://s3.us-west-2.amazonaws.com/data.ldn.auspatious.com/ausp_ls_geomad/0-0-2/ausp_ls_geomad.parquet\" # Pacific\n",
    "stac_docs = search_sync(stac_geoparquet, intersects=aoi_geojson, datetime=datetime_year) # , store=S3Store(bucket=\"data.ldn.auspatious.com\", region=\"us-west-2\"))\n",
    "print(f\"Found {len(stac_docs)} STAC documents for this AOI and datetime year\")\n",
    "geomad_items = [Item.from_dict(feature) for feature in stac_docs]\n",
    "print(f\"Parsed {len(geomad_items)} STAC items\")\n",
    "\n",
    "bands = list(geomad_items[0].assets.keys())\n",
    "print(f\"Available bands: {bands}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361586d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load geomad without any CRS/res options. We want to load it as native.\n",
    "geomad_ds = load(\n",
    "    geomad_items,\n",
    "    crs=analysis_crs,\n",
    "    resolution=10, # Do 100 for faster development, but use 10m for final runs.\n",
    "    groupby=\"solar_day\",\n",
    "    intersects=aoi_geojson,\n",
    "    datetime=datetime_year,\n",
    "    chunks={}, # Force lazy loading.\n",
    ")\n",
    "geomad_ds = geomad_ds.squeeze().load() # .load() to read the data into memory, .squeeze() to remove any singleton dimensions.\n",
    "print(f\"GeoMAD dataset shape: {geomad_ds.dims}\")\n",
    "geomad_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785dd6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "geomad_ds.odc.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(geomad_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7bf8a6",
   "metadata": {},
   "source": [
    "# Scale observed values\n",
    "\n",
    "Done before calculating indices.\n",
    "\n",
    "Apply scaling to the relevant bands -- Landsat data values are supplied as integers, but it is best practice to scale them to have values between 0 and 1 when doing calculations of indices. Landsat has a scale factor of 0.0000275 and an offset of -0.2. The cell below defines a function for this, and also sets any negative values to nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1808af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bands.remove('count')\n",
    "except ValueError:\n",
    "    pass # If rerunning, count is already removed\n",
    "band_names_geomad = [b for b in bands if b.endswith('mad')] # ['smad', 'bcmad', 'emad']\n",
    "band_names_geomedian = [b for b in bands if b not in band_names_geomad] # ['red', 'blue', 'green', 'swir16', 'swir22', 'nir08']\n",
    "print(band_names_geomad)\n",
    "print(band_names_geomedian)\n",
    "\n",
    "# Adapted from https://github.com/auspatious/cloud-native-geospatial-eo-workshop/blob/main/02_Cloud_Native_Land_Productivity_For_SDG15_LS.ipynb\n",
    "def scale_offset(band):\n",
    "    nodata = band == 0\n",
    "    # Apply a scaling factor and offset to the band\n",
    "    band = band * 0.0000275 + -0.2\n",
    "    band = band.clip(0, 1)\n",
    "\n",
    "    return band.where(~nodata, other=np.nan)\n",
    "\n",
    "# Only scale the integer geomedian bands, not the float MAD bands\n",
    "for band in band_names_geomedian:\n",
    "    geomad_ds[band] = scale_offset(geomad_ds[band])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geomad_ds\n",
    "print(\"red range: \", geomad_ds['red'].min().values, geomad_ds['red'].max().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702e2476",
   "metadata": {},
   "source": [
    "# Indices\n",
    "\n",
    "### Calculate indices like NDVI (from the Geomedian/GeoMAD bands)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/digitalearthpacific/dep-sdb/blob/main/src/utils.py\n",
    "# Use our own function because deafrica calculate_indices is large and clunky.\n",
    "def make_indices(geomad: xr.Dataset) -> xr.Dataset:\n",
    "    nir = geomad.nir08\n",
    "    red = geomad.red\n",
    "    green = geomad.green\n",
    "    blue = geomad.blue\n",
    "    swir1 = geomad.swir16\n",
    "    swir2 = geomad.swir22\n",
    "\n",
    "    # Vegetation\n",
    "    geomad[\"ndvi\"] = (nir - red) / (nir + red)\n",
    "\n",
    "    # Water\n",
    "    geomad[\"ndwi\"] = (green - nir) / (green + nir)\n",
    "    geomad[\"mndwi\"] = (green - swir1) / (green + swir1)\n",
    "\n",
    "    # Turbidity\n",
    "    geomad[\"ndti\"] = (red - green) / (red + green)\n",
    "\n",
    "    # Soil\n",
    "    # Bare Soil Index (common form)\n",
    "    geomad[\"bsi\"] = ((swir1 + red) - (nir + blue)) / ((swir1 + red) + (nir + blue))\n",
    "    # Modified Bare Soil Index\n",
    "    geomad[\"mbi\"] = ((swir1 - swir2 - nir) / (swir1 + swir2 + nir)) + 0.5\n",
    "\n",
    "    # Built-Up\n",
    "    # Built-up Area Extraction Index (BAEI)\n",
    "    geomad[\"baei\"] = (red + 0.3) / (green + swir1)\n",
    "    # Built-Up Index (BUI = NDBI - NDVI)\n",
    "    ndbi = (swir1 - nir) / (swir1 + nir)\n",
    "    geomad[\"bui\"] = ndbi - geomad[\"ndvi\"]\n",
    "\n",
    "\n",
    "    # Shallow water bathymetry. Need to calc it before scaling aparently\n",
    "    # # Stumpf, calculate off non-scaled data to remove nan/infinities\n",
    "    # geomad[\"stumpf\"] = np.log(np.abs(geomad.green - geomad.blue)) / np.log(\n",
    "    #     geomad.green + geomad.blue\n",
    "    # )\n",
    "\n",
    "    # Blue over green index\n",
    "    geomad[\"bg\"] = blue / green\n",
    "\n",
    "    # Blue over red index\n",
    "    # geomad[\"br\"] = blue / red\n",
    "\n",
    "    # Natural log of blue/green\n",
    "    geomad[\"ln_bg\"] = np.log(blue / green)\n",
    "\n",
    "    # # Lyzenga... seems problematic\n",
    "    # geomad[\"lyzenga\"] = np.log(green / blue)\n",
    "\n",
    "    return geomad\n",
    "\n",
    "geomad_ds = make_indices(geomad_ds)\n",
    "geomad_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50650702",
   "metadata": {},
   "source": [
    "# Get Elevation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a051a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DEM\n",
    "dem_items = list(\n",
    "    catalog.search(\n",
    "        collections=[\"cop-dem-glo-30\"],\n",
    "        intersects=aoi_geojson,\n",
    "    ).items()\n",
    ")\n",
    "\n",
    "print(f\"Found {len(dem_items)} DEM items\")\n",
    "print(dem_items[0].to_dict() if dem_items else \"NO ITEMS FOUND\")\n",
    "\n",
    "# Upscales from 30m to 10m.\n",
    "dem = load(\n",
    "    dem_items,\n",
    "    # geobox=geomad_ds.odc.geobox, # Use the same geobox as geomad for alignment. This will reproject and resample the DEM to match geomad.\n",
    "    like=geomad_ds,\n",
    "    resampling=\"bilinear\",\n",
    "    patch_url=sign_url,\n",
    ").squeeze().compute().rename({\"data\": \"elevation\"})\n",
    "\n",
    "print(dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ce619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to clip or mask. It will get sampled to points anyway.\n",
    "# dem = dem.rio.clip(aoi.to_crs(analysis_crs).geometry, crs=analysis_crs)\n",
    "dem[\"elevation\"].odc.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89498f8e",
   "metadata": {},
   "source": [
    "# Add Geomedian, GeoMAD, and elevation data to training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff68170",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(geomad_ds)\n",
    "print(dem)\n",
    "\n",
    "dem = dem.assign_coords(time=geomad_ds.time)\n",
    "geomad_dem = xr.merge([geomad_ds, dem])\n",
    "print(\"Red:\", geomad_dem['red'].min().values, geomad_dem['red'].max().values)\n",
    "print(\"NDVI:\", geomad_dem['ndvi'].min().values, geomad_dem['ndvi'].max().values)\n",
    "print(\"Elevation:\", geomad_dem['elevation'].min().values, geomad_dem['elevation'].max().values)\n",
    "print(geomad_dem)\n",
    "# geomad_dem = geomad_dem.fillna(0) # Fill NaN with 0 for exploration. We will keep NaN in the actual training data.\n",
    "geomad_dem.odc.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97fce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data for model\n",
    "# geomad_dem.rio.to_raster(\"geomad_dem.tif\", dtype='uint8', compress='deflate')\n",
    "geomad_dem.to_netcdf(\"geomad_dem.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f5b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_names_with_indices = [v for v in geomad_dem.data_vars]\n",
    "print(band_names_with_indices)\n",
    "\n",
    "# Extract Geomedian and GeoMAD band values at each sample point location\n",
    "\n",
    "# Ensure both datasets are in the same CRS (6933)\n",
    "# TODO: Reproject sample points to match native CRS of geomad before sampling.\n",
    "gpd_random_samples_6933 = gpd_random_samples.to_crs(analysis_crs)\n",
    "\n",
    "# Build point-wise x/y arrays with the SAME 'points' index\n",
    "points_idx = np.arange(len(gpd_random_samples_6933))\n",
    "xs = xr.DataArray(gpd_random_samples_6933.geometry.x.values, dims=\"points\", coords={\"points\": points_idx})\n",
    "ys = xr.DataArray(gpd_random_samples_6933.geometry.y.values, dims=\"points\", coords={\"points\": points_idx})\n",
    "# Point-wise nearest sampling (one sampled pixel per input point)\n",
    "sampled = geomad_dem[band_names_with_indices].sel(x=xs, y=ys, method=\"nearest\")\n",
    "\n",
    "# Convert to dataframe indexed by points; keep exactly one row per point\n",
    "sampled_df = sampled.to_dataframe().reset_index()\n",
    "sampled_df = sampled_df.sort_values(\"points\").reset_index(drop=True)\n",
    "\n",
    "# Merge back to training points (original CRS)\n",
    "training_data = gpd_random_samples.reset_index(drop=True).copy()\n",
    "training_data = pd.concat([training_data, sampled_df[band_names_with_indices]], axis=1)\n",
    "\n",
    "print(f\"Training data shape: {training_data.shape}\")\n",
    "print(\"Unique values per band:\")\n",
    "print(training_data[band_names_with_indices].nunique())\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc3b582",
   "metadata": {},
   "source": [
    "## Visualise GeoMedian red values per sample point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4207edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise red values using the actual data range\n",
    "training_data.explore(\n",
    "    column=\"red\",\n",
    "    cmap=\"Reds\",\n",
    "    k=7,\n",
    "    scheme=\"natural_breaks\",\n",
    "    legend=True,\n",
    "    style_kwds={\"radius\": 4, \"fillOpacity\": 0.8, \"weight\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4587ff4",
   "metadata": {},
   "source": [
    "## Explore elevation data on training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d1a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.explore(\n",
    "    column=\"elevation\",\n",
    "    cmap=\"Blues\",\n",
    "    scheme=\"natural_breaks\",\n",
    "    k=7,\n",
    "    legend=True,\n",
    "    style_kwds={\"radius\": 4, \"fillOpacity\": 0.8, \"weight\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed7ff1",
   "metadata": {},
   "source": [
    "## Explore NDVI data on training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31526acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.explore(\n",
    "    column=\"ndvi\",\n",
    "    cmap=\"Greens\",\n",
    "    scheme=\"quantiles\",\n",
    "    k=7,\n",
    "    legend=True,\n",
    "    style_kwds={\"radius\": 4, \"fillOpacity\": 0.8, \"weight\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80636ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data = training_data.drop(columns=[\"time\", 'spatial_ref', 'count'])\n",
    "\n",
    "# Write the final training data\n",
    "out_data.to_file(out_fname, driver=\"GeoJSON\", index=False)\n",
    "# out_data.to_file(out_fname.replace(\".geojson\", \".gpkg\"), driver=\"GPKG\")\n",
    "# out_data.to_csv(out_fname.replace(\".geojson\", \".csv\"), index=False)\n",
    "\n",
    "print(f\"Saved training data as geojson, CSV, and geopackage to {out_fname}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldn-TK5rT0MB-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
